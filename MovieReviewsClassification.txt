# -*- coding: utf-8 -*-
"""HW1MovieReviewsClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LA_VJiApxscyB1P8D3yCPWNRIMMdPAu-

# Install Required Libraries
"""

# for removing non-english words using enchant
!apt install -qq enchant
!pip install pyenchant

# for removing contractions
!pip install contractions==0.0.18

# for using Beautiful Soup to remove HTML tags
!pip install bs4

#for ordered set
!pip install ordered-set

"""#Import Required Libraries"""

import pandas as pd
import numpy as np
import nltk
import contractions
import matplotlib.pyplot as plt
import seaborn as sns
import enchant

from bs4 import BeautifulSoup
from pandas.core.frame import DataFrame
from ordered_set import OrderedSet
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.metrics.pairwise import manhattan_distances
from sklearn.metrics import accuracy_score
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet

nltk.download("punkt")
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

"""# Reading the data from the .txt file

I used read_csv/read_table but there were tokenizing errors. Then, I tried reading using readLines().
"""

trainingData = []
testData = []
with open("training_data.txt", "r") as trainFile:
  trainingData = trainFile.readlines()
with open("test_data.txt", "r") as testFile:
  testData = testFile.readlines()

sentiments = []
reviews = []
for i in trainingData:
  sentiments.append(i.split('\t')[0])
  reviews.append(i.split('\t')[1])

"""# Split Data"""

x_train, x_test, y_train, y_test = train_test_split(reviews, sentiments, train_size = 0.90)

dfTrain = pd.DataFrame()
dfTrain['sentiments'] = y_train
dfTrain['reviews'] = x_train
dfTrain.head()

dfTest = pd.DataFrame()
dfTest['sentiments'] = y_test
dfTest['reviews'] = x_test
dfTest.head()

"""Checking if any null value exist"""

# checking for null values
print("checking train data: ",dfTrain.isna().sum())
print("checking test data: ",dfTest.isna().sum())

"""# Data Pre-Processing"""

sns.countplot(dfTrain['sentiments'])

#converting to lower case
dfTrain['clean_reviews'] = dfTrain['reviews'].str.lower()
dfTest['clean_reviews'] = dfTest['reviews'].str.lower()

dfTrain.head()

dfTest.head()

def get_wordnet_pos(tag):
  if tag.startswith('J'):
    return wordnet.ADJ
  elif tag.startswith('V'):
    return wordnet.VERB
  elif tag.startswith('N'):
    return wordnet.NOUN
  elif tag.startswith('R'):
    return wordnet.ADV
  else:
    return wordnet.NOUN

def removeHTMLtags(x):
  beautifulSoup = BeautifulSoup(x, "html.parser")
  for data in beautifulSoup(['style', 'script']):
    data.decompose()
  return ' '.join(beautifulSoup.stripped_strings)

# creating function for cleaning data
def dataCleaning(df, content_field):

  #removing html tags
  df[content_field] = df[content_field].apply(lambda x: removeHTMLtags(x))
  
  #removing contractions
  dfTrain[content_field] = dfTrain[content_field].apply(lambda x: ' '.join(contractions.fix(word) for word in x.split()))
  
  #removing non-alphabet characters
  df[content_field] = df[content_field].replace(to_replace = '([^a-zA-Z])', value = " ", regex= True)

  #remove duplicate words in the review
  df[content_field] = df[content_field].apply(lambda x: ' '.join(word for word in OrderedSet(x.split())))

  #Remove Stop Words
  stop = set(stopwords.words('english'))
  df[content_field] = df[content_field].apply(lambda x: ' '.join(x for x in x.split() if x not in stop))

  #tokenizing the data
  df[content_field] = df[content_field].apply(word_tokenize)

  #adding part of speech tags
  df[content_field] = df[content_field].apply(nltk.tag.pos_tag)

  #converting parts of speech tag to wordnet format
  df[content_field] = df[content_field].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])

  #lemmatize
  lemmatizer = WordNetLemmatizer()
  df[content_field] = df[content_field].apply(lambda x: [lemmatizer.lemmatize(word, tag) for word, tag in x])

  #removing non-english words
  d = enchant.Dict("en_US")
  df[content_field] = df[content_field].apply(lambda x: " ".join(word for word in x if d.check(word)))

dataCleaning(dfTrain,"clean_reviews")
dfTrain.head()

dataCleaning(dfTest,"clean_reviews")
dfTest.head()

train_Y = dfTrain['sentiments'].to_numpy()
train_X = dfTrain['clean_reviews'].to_numpy()
test_X = dfTest['clean_reviews'].to_numpy()
test_Y = dfTest['sentiments'].to_numpy()

"""# TfIdf Vectorization"""

tfidfVectorizer = TfidfVectorizer(ngram_range=(1, 3))

#vectorize train data (split)
trainReviewsVectorized = tfidfVectorizer.fit_transform(train_X)

#vectorize test data (split)
testReviewsVectorized = tfidfVectorizer.transform(test_X)

len(tfidfVectorizer.get_feature_names())

"""# Chi2 feature selection (dimensionality reduction)"""

chi2score = chi2(trainReviewsVectorized, train_Y)[1]

ch2_result = []
n = 20000
ch2 = SelectKBest(chi2,k = n)
x_train_chi2_selected = ch2.fit_transform(trainReviewsVectorized, train_Y)
x_test_chi2_selected = ch2.transform(testReviewsVectorized)

print("len of train vector matrix after dimension reduction: ",x_train_chi2_selected.get_shape())
print("len of test vector matrix after dimension reduction: ",x_test_chi2_selected.get_shape())

"""# KNN implementation and Accuracy Report"""

def predictSentiment(kNearestNeighbours, sentiments):
    positiveSentiment, negativeSentiment = 0, 0
    for nearestNeighbour in kNearestNeighbours:
        if int(sentiments[nearestNeighbour]) == 1:
            positiveSentiment += 1
        else:
            negativeSentiment += 1
    return "+1" if (positiveSentiment >= negativeSentiment) else "-1"

def knnClassification(trainVectorMatrix, testVectorMatrix):
    cosineSimilaritiesOfReviews = cosine_similarity(testVectorMatrix, trainVectorMatrix)
    #nearest neighbours to be checked
    k = 400
    testSentiments = list()
    for reviewSimilarity in cosineSimilaritiesOfReviews:
      kNN = np.argsort(-reviewSimilarity)[:k]
      prediction = predictSentiment(kNN, train_Y)
      testSentiments.append(prediction)
    return testSentiments

"""Using Euclidean Distance"""

def euclidDistance(trainVectorMatrix, testVectorMatrix):
    euclideanDist = euclidean_distances(testVectorMatrix, trainVectorMatrix)
    #nearest neighbours to be checked
    k = 400
    testSentiments = list()
    for dist in euclideanDist:
      kNN = np.argsort(-dist)[:k]
      prediction = predictSentiment(kNN, train_Y)
      testSentiments.append(prediction)
    return testSentiments

"""Manhattan Distance"""

def manhattanDistance(trainVectorMatrix, testVectorMatrix):
    manhattanDistance = manhattan_distances(testVectorMatrix, trainVectorMatrix)
    #nearest neighbours to be checked
    k = 400
    testSentiments = list()
    for dist in manhattanDistance:
      kNN = np.argsort(-dist)[:k]
      prediction = predictSentiment(kNN, train_Y)
      testSentiments.append(prediction)
    return testSentiments

type(test_Y)

resultCosine = knnClassification(x_train_chi2_selected, x_test_chi2_selected)
resultEuclidean = euclidDistance(x_train_chi2_selected, x_test_chi2_selected)
resultManhattan = manhattanDistance(x_train_chi2_selected, x_test_chi2_selected)

predCosine = accuracy_score(resultCosine, test_Y)
predEuclidean = accuracy_score(resultEuclidean, test_Y)
predManhattan = accuracy_score(resultManhattan, test_Y)

print("accuracy with cosine similarity: ", predCosine)
print("accuracy with euclidean distances: ", predEuclidean)
print("accuracy with euclidean distances: ", predManhattan)

# creating the dataset
data = {'Cosine':predCosine, 'Euclidean':predEuclidean, 'Manhattan':predManhattan}
algorithm = list(data.keys())
values = list(data.values())
  
fig = plt.figure(figsize = (10, 5))
 
# creating the bar plot
plt.bar(algorithm, values, width = 0.4)

"""# Pre Processing Test Data"""

dfTestData = pd.DataFrame()
dfTestData['reviews'] = testData
dfTestData.head()

#converting to lower case
dfTestData['clean_reviews'] = dfTestData['reviews'].str.lower()

dataCleaning(dfTestData,"clean_reviews")
dfTest.head()

testDataReviewsVectorized = tfidfVectorizer.transform(dfTestData['clean_reviews'])

x_Data_test_chi2_selected = ch2.transform(testDataReviewsVectorized)

resultFinal = knnClassification(x_train_chi2_selected, x_Data_test_chi2_selected)

sns.countplot(resultFinal)

"""# Write to output file"""

sentimentFileWriter = open("output.txt", "w")
sentimentFileWriter.writelines("%s\n" % sentiment for sentiment in resultFinal)
sentimentFileWriter.close()

"""# Tries to improve the accuracy

**Dimensionality reduction using singular value decomposition**

not much effect. With less number of features, performance even reduced.
"""

from sklearn.decomposition import TruncatedSVD
svd =  TruncatedSVD(n_components = 5000)
x_train_svd = svd.fit_transform(x_train_chi2_selected)

x_test_svd = svd.transform(x_test_chi2_selected)

"""**Hashing Vectorizer**

performance dropped to 71.6%
"""

from sklearn.feature_extraction.text import HashingVectorizer
hashVectorizer = HashingVectorizer(ngram_range=(1, 3))
trainReviewsVectorized = hashVectorizer.fit_transform(dfTrain['clean_reviews'])

"""Using another model, got an accuracy around 87%. """

train_Y.shape

from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score

classifier = LinearSVC()
classifier.fit(x_train_chi2_selected, train_Y)
y_pred = classifier.predict(x_test_chi2_selected)
acc = accuracy_score(y_pred, test_Y)
print("Accuracy Score of SVC:", acc*100,"%")