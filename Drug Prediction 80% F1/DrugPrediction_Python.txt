# -*- coding: utf-8 -*-
"""HW2DrugPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hcq5F631NMEXH9mE0ZqBikDhenlxHKx3

# Import Required Libraries
"""

import pandas as pd
import numpy as np

from sklearn import preprocessing
from datetime import datetime

from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

from sklearn.decomposition import TruncatedSVD
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.ensemble import AdaBoostClassifier

from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt

start = datetime.now()

"""# Data Retrieval and Preprocessing"""

trainingData = []
testData = []
with open("train.txt", "r") as trainFile:
  trainingData = trainFile.readlines()
with open("test.txt", "r") as testFile:
  testData = testFile.readlines()

sentiments = []
reviews = []
for i in trainingData:
  sentiments.append(i.split('\t')[0])
  reviews.append(i.split('\t')[1])

for i in range(len(sentiments)):
  sentiments[i] = int(sentiments[i])

trainDataInput = pd.DataFrame()
trainSentiments = pd.DataFrame()
trainDataInput['features'] = reviews
trainSentiments['sentiments'] = sentiments
trainDataInput.head()

trainSentiments.value_counts()

# splitting the feature index from the input and replace NaN value with 0
trainData = trainDataInput.features.str.split(expand=True).astype(float)
trainData.apply(lambda x: x.replace(np.NaN, 0, inplace = True))
trainData.head()

# getting max value of feature index
trainData.max().max()

# converting the sparse representation to full matrix
trainDataArray = trainData.to_numpy()
trainDataFull = [[0 for x in range(100000)] for y in range(800)] 
for i in range(800):
  for j in range(6061):
    x = int(trainDataArray[i][j])
    if x is not 0:
      trainDataFull[i][x-1] = 1

# representing data in Data Frame
dfTrain = pd.DataFrame(trainDataFull)
dfTrain.head()

"""# Feature Selection


"""

trainDataNumpy = np.array(trainDataFull)
trainDataNumpy.shape

trainSentimentNumpy = trainSentiments['sentiments'].to_numpy()
trainSentimentNumpy.shape

# feature selection using SVD
svd_trunc = TruncatedSVD(n_components=800, random_state=27, n_iter = 30)
trainDataSVD = svd_trunc.fit_transform(trainDataNumpy, trainSentimentNumpy)
pd.DataFrame(trainDataSVD).head()

"""# Split into train and validation set and Predict validation set"""

# Split into train and validation set
x_train , x_test, y_train, y_test = train_test_split(trainDataSVD, trainSentimentNumpy, random_state=27, train_size=0.8)

#classifying using linear SVM
classifier = LinearSVC(random_state = 27)
classifier.fit(x_train, y_train)
y_predSVC = classifier.predict(x_test)
y_predSVC

#checking count of predicted 1's and 0's
print(pd.DataFrame(y_predSVC).value_counts())

#classifying using logistic regression
logisticRegr = LogisticRegression(random_state = 27, class_weight={0:1, 1:3000}, C=0.2)
logisticRegr.fit(x_train, y_train)
y_predLogistic = logisticRegr.predict(x_test)
y_predLogistic

#checking count of predicted 1's and 0's
pd.DataFrame(y_predLogistic).value_counts()

decisionTree = DecisionTreeClassifier(class_weight={0: 1, 1: 1.5}, random_state = 27)
decisionTree.fit(x_train,y_train)
y_predDT = decisionTree.predict(x_test)
y_predDT

#checking count of predicted 1's and 0's
pd.DataFrame(y_predDT).value_counts()

rfc = RandomForestClassifier(random_state = 27, max_depth = 8)
rfc.fit(x_train, y_train)
y_predRFC = rfc.predict(x_test)
y_predRFC

#checking count of predicted 1's and 0's
pd.DataFrame(y_predRFC).value_counts()

abc = AdaBoostClassifier(base_estimator= DecisionTreeClassifier(class_weight= {0:1, 1:1.5}))
adaBoost = abc.fit(x_train, y_train)
y_predAB = adaBoost.predict(x_test)
y_predAB

#checking count of predicted 1's and 0's
pd.DataFrame(y_predAB).value_counts()

f1DT = f1_score(y_test, y_predDT)
recallDT = recall_score(y_test, y_predDT)

f1RFC = f1_score(y_test, y_predRFC)
recallRFC = recall_score(y_test, y_predRFC)

f1Logistic = f1_score(y_test, y_predLogistic)
recallLogistic = recall_score(y_test, y_predLogistic)

f1SVC = f1_score(y_test, y_predSVC)
recallSVC = recall_score(y_test, y_predSVC)

f1AB = f1_score(y_test, y_predAB)
recallAB = recall_score(y_test, y_predAB)

print("Recall: Decision Tree: ",recallDT)
print("F1: Decision Tree: ",f1DT)

print("Recall: RFC: ",recallRFC)
print("F1: RFC: ",f1RFC)

print("Recall: Logistic Regression: ",recallLogistic)
print("F1: Logistic Regression: ",f1Logistic)

print("Recall: SVM: ",recallSVC)
print("F1: SVM: ",f1SVC)

print("Recall: Adaboost: ",recallAB)
print("F1: Adaboost: ",f1AB)

# creating the dataset
data = {'Decision_Tree':f1DT, 'RFC':f1RFC, 'Logictic':f1Logistic, 'SVM':f1SVC, 'AB':f1AB}
algorithm = list(data.keys())
values = list(data.values())
  
fig = plt.figure(figsize = (10, 5))
plt.title("Fig:5 F1 Score") 

# creating the bar plot
plt.bar(algorithm, values, width = 0.2)
plt.show()

# creating the dataset
data = {'Decision_Tree':recallDT, 'RFC':recallRFC, 'Logictic':recallLogistic, 'SVM':recallSVC, 'AB':recallAB }
algorithm = list(data.keys())
values = list(data.values())
  
fig = plt.figure(figsize = (10, 5))
plt.title("Recall Score") 

# creating the bar plot
plt.bar(algorithm, values, width = 0.2)
plt.show()

accuracyLogistic = accuracy_score(y_predLogistic, y_test)
accuracySVM = accuracy_score(y_predSVC, y_test)
accuracyDT = accuracy_score(y_predDT, y_test)
accuracyRFC = accuracy_score(y_predRFC, y_test)
accuracyAB = accuracy_score(y_predAB, y_test)

print("Accuracy with Decision Tree: ", accuracyDT)
print("Accuracy with RFC Tree: ", accuracyRFC)
print("Accuracy with Logistic Regression: ", accuracyLogistic)
print("Accuracy with Linear SVM: ", accuracySVM)
print("Accuracy with Adaboost: ", accuracyAB)

# creating the dataset
data = {'DT':accuracyDT, 'RFC':accuracyRFC, 'Logictic_Regression':accuracyLogistic, 'SVM':accuracySVM, 'AB':accuracyAB}
algorithm = list(data.keys())
values = list(data.values())

fig = plt.figure(figsize = (10, 5))
plt.title("Fig:6 Accuracy") 

# creating the bar plot
plt.bar(algorithm, values, width = 0.2)

"""# Test Data Processing"""

testDataInput = pd.DataFrame()
testDataInput['features'] = testData
testDataInput.head()

testData = testDataInput['features'].str.split(expand=True).astype(float)
testData.apply(lambda x: x.replace(np.NaN, 0, inplace = True))
testData.head()

testDataArray = testData.to_numpy()
testDataFull = [[0 for x in range(100000)] for y in range(350)]
for i in range(350):
  for j in range(4857):
    x = int(testDataArray[i][j])
    if x is not 0:
      testDataFull[i][x-1] = 1

dfTest = pd.DataFrame(testDataFull)
dfTest.tail()

#SVD dimensionality reduction
testDataReduced = svd_trunc.transform(testDataFull)

y_predTestLogistic = logisticRegr.predict(testDataReduced)
#checking count of predicted 1's and 0's
pd.DataFrame(y_predTestLogistic).value_counts()

y_predTestSVC = classifier.predict(testDataReduced)
pd.DataFrame(y_predTestSVC).value_counts()

y_predTestDT = decisionTree.predict(testDataReduced)
pd.DataFrame(y_predTestDT).value_counts()

y_predTestRFC = rfc.predict(testDataReduced)
pd.DataFrame(y_predTestRFC).value_counts()

y_predTestAB = adaBoost.predict(testDataReduced)
pd.DataFrame(y_predTestAB).value_counts()

"""# Write Prediction to File"""

sentimentFileWriter = open("output.txt", "w")
sentimentFileWriter.writelines("%s\n" % sentiment for sentiment in y_predTestLogistic)
sentimentFileWriter.close()

end = datetime.now()
diff = end-start
print("Total Runtime in seconds: ",diff.seconds)
print("Total Runtime in minutes: ",diff.seconds/60)

"""# Identifying best Hyper Parameters

Logistic Regression
"""

#classifying using logistic regression
C = [0.2,0.4,0.6,0.8,1]
y_predLogisticList = []
for i in C:
  logisticRegr = LogisticRegression(random_state = 27, class_weight={0:1, 1:3000}, C=i)
  logisticRegr.fit(x_train, y_train)
  y_predLogisticList.append(logisticRegr.predict(x_test))
  
f1LogisticList = []
for pred in y_predLogisticList:
  f1LogisticList.append(f1_score(y_test, pred))
# creating the dataset
data = {'0.2':f1LogisticList[0], '0.4':f1LogisticList[1], '0.6':f1LogisticList[2], '0.8':f1LogisticList[3], '1.0':f1LogisticList[4]}
algorithm = list(data.keys())
values = list(data.values())
  
fig = plt.figure(figsize = (10, 5))
plt.title("Fig:1") 
plt.xlabel("Value of C")
plt.ylabel("F1-Score")

# creating the bar plot
plt.bar(algorithm, values, width = 0.2)
plt.show()

"""Decision Tree"""

maxFeatures = [10,20,50,100,200, None]
yPredDTList = []
for i in maxFeatures:
  decisionTree = DecisionTreeClassifier(class_weight={0: 1, 1: 1.5}, random_state = 27, max_features=i)
  decisionTree.fit(x_train,y_train)
  yPredDTList.append(decisionTree.predict(x_test))

f1DecisionTreeList = []
for pred in yPredDTList:
  f1DecisionTreeList.append(f1_score(y_test, pred))

# creating the dataset
data = {'10':f1DecisionTreeList[0], '20':f1DecisionTreeList[1], '50':f1DecisionTreeList[2], '100':f1DecisionTreeList[3], '200':f1DecisionTreeList[4], 'None': f1DecisionTreeList[5]}
algorithm = list(data.keys())
values = list(data.values())
  
fig = plt.figure(figsize = (10, 5))
plt.title("Fig:2") 
plt.xlabel("Value of max_features")
plt.ylabel("F1-Score")

# creating the bar plot
plt.bar(algorithm, values, width = 0.2)
plt.show()

"""Random Forest Classifier"""

maxFeatures = [20,50,100,200,400,None]
yPredRFCList = []
for i in maxFeatures:
  rfc = RandomForestClassifier(random_state = 27, max_depth = 8, max_features=i)
  rfc.fit(x_train, y_train)
  yPredRFCList.append(rfc.predict(x_test))

f1RFCList = []
for pred in yPredRFCList:
  f1RFCList.append(f1_score(y_test, pred))

# creating the dataset
data = {'20':f1RFCList[0], '50':f1RFCList[1], '100':f1RFCList[2], '200':f1RFCList[3], '400':f1RFCList[4], 'None': f1RFCList[5]}
algorithm = list(data.keys())
values = list(data.values())
  
fig = plt.figure(figsize = (10, 5))
plt.title("Fig:3") 
plt.xlabel("Value of max_features")
plt.ylabel("F1-Score")

# creating the bar plot
plt.bar(algorithm, values, width = 0.2)
plt.show()

"""Adaboost"""

n_estimators = [5,10,20,50,100]
yPredABList = []
for i in n_estimators:
  abc = AdaBoostClassifier(n_estimators=i, base_estimator= DecisionTreeClassifier(class_weight= {0:1, 1:1.5}))
  adaBoost = abc.fit(x_train, y_train)
  yPredABList.append(adaBoost.predict(x_test))

f1ABList = []
for pred in yPredRFCList:
  f1ABList.append(f1_score(y_test, pred))

# creating the dataset
data = {'5':f1ABList[0], '10':f1ABList[1], '20':f1ABList[2], '50':f1ABList[3], '100':f1ABList[4]}
algorithm = list(data.keys())
values = list(data.values())
  
fig = plt.figure(figsize = (10, 5))
plt.title("Fig:4") 
plt.xlabel("n_estimators")
plt.ylabel("F1-Score")

# creating the bar plot
plt.bar(algorithm, values, width = 0.2)
plt.show()

"""# Improving F1-Score

Trying to improving F1 score using Over Sampling
"""

#SMOTE oversampling
from imblearn.over_sampling import SMOTE
oversample = SMOTE(random_state = 27, sampling_strategy=0.5)
smoteX, smoteY = oversample.fit_resample(trainDataNumpy, trainSentimentNumpy)

pd.DataFrame(smoteY).value_counts()

#shuffle
from sklearn.utils import shuffle
X_shuffle, Y_shuffle = shuffle(smoteX, smoteY, random_state=27)

x_trainImp, x_testImp, y_trainImp, y_testImp = train_test_split(X_shuffle, Y_shuffle, train_size = 0.80, stratify = Y_shuffle, random_state = 27)

print("train count: ",pd.DataFrame(y_trainImp).value_counts())
print("test count: ",pd.DataFrame(y_testImp).value_counts())

# feature selection using SVD
trainDataSVDImp = svd_trunc.transform(x_trainImp)
testDataSVDImp = svd_trunc.transform(x_testImp)
pd.DataFrame(trainDataSVDImp).head()

logisticRegImp = LogisticRegression(random_state = 27)
logisticRegImp.fit(trainDataSVDImp, y_trainImp)
y_predLogImp = logisticRegImp.predict(testDataSVDImp)
pd.DataFrame(y_predLogImp).value_counts()

decisionTreeImp = DecisionTreeClassifier(class_weight={0: 1, 1: 5}, random_state = 27)
decisionTreeImp.fit(trainDataSVDImp, y_trainImp)
y_predDTImp = decisionTreeImp.predict(testDataSVDImp)
pd.DataFrame(y_predDTImp).value_counts()

rfcImp = RandomForestClassifier(random_state = 27)
rfcImp.fit(trainDataSVDImp, y_trainImp)
y_predRFCImp = rfcImp.predict(testDataSVDImp)
pd.DataFrame(y_predRFCImp).value_counts()

#classifying using linear SVM
classifierImp = LinearSVC(random_state = 27)
classifierImp.fit(trainDataSVDImp, y_trainImp)
y_predSVCImp = classifierImp.predict(testDataSVDImp)
pd.DataFrame(y_predSVCImp).value_counts()

f1DTImp = f1_score(y_testImp, y_predDTImp)
f1RFCImp = f1_score(y_testImp, y_predRFCImp)
f1LogisticImp = f1_score(y_testImp, y_predLogImp)
f1SVCImp = f1_score(y_testImp, y_predSVCImp)

print("F1: Decision Tree: ",f1DTImp)
print("F1: RFC: ",f1RFCImp)
print("F1: Logistic Regression: ",f1LogisticImp)
print("F1: SVM: ",f1SVCImp)

accuracyLogisticImp = accuracy_score(y_predLogImp, y_testImp)
accLinearSVMImp = accuracy_score(y_predSVCImp, y_testImp)
accuracyDTImp = accuracy_score(y_predDTImp, y_testImp)
accuracyRFCImp = accuracy_score(y_predRFCImp, y_testImp)
print("Accuracy with Logistic Regression: ", accuracyLogisticImp)
print("Accuracy with Linear SVM: ", accLinearSVMImp)
print("Accuracy with Decision Tree: ", accuracyDTImp)
print("Accuracy with RFC Tree: ", accuracyRFCImp)

"""predicting actual test data using decision tree trained on weighted tree after oversampling"""

y_predTestDTImp = decisionTreeImp.predict(testDataReduced)
pd.DataFrame(y_predTestDTImp).value_counts()

sentimentFileWriterImp = open("outputImp.txt", "w")
sentimentFileWriterImp.writelines("%s\n" % sentiment for sentiment in y_predTestDTImp)
sentimentFileWriterImp.close()